services:
  postgres:
    image: postgres:16-alpine
    container_name: postgres_dev
    restart: always
    env_file:
      - .env.dev
    ports:
      - "5432:5432"
    volumes:
      - pgdata_dev:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    container_name: redis_dev
    restart: always
    ports:
      - "6379:6379"
    env_file:
      - .env.dev

  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: ts_app_dev
    depends_on:
      - postgres
      - redis
    env_file:
      - .env.dev
    ports:
      - "3000:3000"
    volumes:
      - .:/src
      - /src/node_modules
    command: npm run dev

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin_dev
    restart: always
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@dev.com
      PGADMIN_DEFAULT_PASSWORD: admin123
      PGADMIN_CONFIG_SERVER_MODE: "False" # allows multiple users locally
    depends_on:
      - postgres
    volumes:
      - pgadmin_data:/var/lib/pgadmin

volumes:
  pgdata_dev:
  pgadmin_data:
# # Pull latest code from main
# git checkout main
# git pull origin main

# # Build the Docker image with latest code
# docker build -t yourusername/nodecrud:latest .

# # Run tests to ensure everything works
# docker run --rm yourusername/nodecrud:latest npm test

# # Push the image to Docker Hub
# docker push yourusername/nodecrud:latest

# ///////////////////////////////////////////////////////////////////////////////////////////////////
# II--> ok let any new develoepr come and pull image will he need to pull code from github or pull form docker directly?
# Perfect ‚Äî this is an important distinction. The answer depends on the purpose of the developer: development (editing code) vs just running the app (testing or deployment).

# 1Ô∏è‚É£ Scenario: New developer wants to develop / change code

# If the developer wants to edit the application, they need the source code.

# They will pull the code from GitHub, not just the Docker image.

# Reason:

# The Docker image contains a snapshot of the code at the time it was built.

# Any changes they make inside the container won‚Äôt automatically update the image.

# For active development, developers mount the code via volumes so changes are live.

# Typical workflow:

# git clone https://github.com/your-org/nodecrud.git
# cd nodecrud
# docker-compose -f docker-compose-dev.yml up --build

# They now have both the latest code and a local dev image.

# 2Ô∏è‚É£ Scenario: New developer wants to just run/test the app

# If the developer just wants to run the latest working app, without changing code:

# They can pull the Docker image directly from Docker Hub:

# docker pull yourusername/nodecrud:latest
# docker run -p 3000:3000 yourusername/nodecrud:latest

# ‚úÖ No need to pull source code from GitHub.

# They are using the prebuilt image which already contains the latest merged code.

# ///////////////////////////////////////////////////////////////////////////////////////////////////
#III -> ok kmeans when new developer come for developemt he pull github code and build compos , so he dont need to do npm i?

# How it works with your Dockerfile and volumes

# Suppose your Dockerfile.dev looks like this (typical for Node.js/TypeScript dev):

# FROM node:20-alpine

# WORKDIR /src

# COPY package*.json ./
# RUN npm install

# COPY . .
# CMD ["npm", "run", "dev"]

# Step by step:

# COPY package*.json ./ ‚Üí copies package.json and package-lock.json into the image.

# RUN npm install ‚Üí installs all dependencies inside the container image.

# COPY . . ‚Üí copies all your source code into the image.

# ‚úÖ After docker-compose up --build, the container already has all npm dependencies installed.

# 2Ô∏è‚É£ Why the developer doesn‚Äôt need npm i locally

# Your Compose file mounts the project folder as a volume:

# volumes:
#   - .:/src
#   - /src/node_modules

# ./src ‚Üí your local code

# /src/node_modules ‚Üí the container keeps its node_modules separate

# This means:

# The container already has dependencies.

# Any new code changes automatically use the installed packages.

# Developer doesn‚Äôt need npm i on their local machine.

# 3Ô∏è‚É£ When might the developer need npm i manually?

# If you are not using Docker volumes, and the developer wants to run the app locally without Docker, they would need npm install.

# If new dependencies are added in package.json, rebuilding the image (--build) will run npm install again inside the container.

# 4Ô∏è‚É£ TL;DR Workflow for a new developer

# Clone GitHub repo

# Copy .env file

# Run:

# docker-compose -f docker-compose-dev.yml up --build

# ‚úÖ Container builds

# ‚úÖ Dependencies installed inside the container

# ‚úÖ Live code updates via volume mount

# ‚ùå No need to run npm install locally

# üí° Industry Practice (MNCs):

# Developers don‚Äôt need Node.js or npm installed locally for dev if using Docker.

# Docker container handles all dependencies.

# Local code changes are reflected instantly via volumes.

# /////////////////////////////////////////////////////////////////////////////////
# IV hw to take dump of postgres?
# To take a dump (backup) of a PostgreSQL database, you use the pg_dump utility that comes with PostgreSQL.

# Here‚Äôs a full breakdown üëá

# üß© 1. Basic Command
# pg_dump -U username -d dbname -f output.sql

# Explanation:

# -U username ‚Üí PostgreSQL user to connect as

# -d dbname ‚Üí name of the database you want to dump

# -f output.sql ‚Üí file to write the dump to (plain SQL format)

# Example:

# pg_dump -U postgres -d mydb -f mydb_backup.sql

# ///////////////////////////////////////////////////////////////////////////////////
# V How to restore .sql or .dump file
# Perfect ‚Äî if you want to restore a PostgreSQL dump using command line (cmd/PowerShell) instead of pgAdmin, you‚Äôll use either psql (for plain .sql dumps) or pg_restore (for .dump/custom-format dumps).

# 1. If you have a .sql dump file

# Run:

# psql -h localhost -U postgres -d testdb -f "C:\path\to\your\dump.sql"

# -h localhost ‚Üí host

# -U postgres ‚Üí user (your .env says postgres)

# -d testdb ‚Üí database name

# -f ‚Üí path to your dump file

# It will ask for your password (Rawat2002 in your case).

# 2. If you have a .dump file (custom format created by pg_dump)

# Run:

# pg_restore -h localhost -U postgres -d testdb "C:\path\to\your\dump.dump"

# Optional flags:

# --clean ‚Üí drops objects before recreating them

# --create ‚Üí creates the database if needed

# Example:

# pg_restore -h localhost -U postgres --clean --create -d postgres "C:\path\to\your\dump.dump"

# This restores into the postgres database and recreates testdb automatically.

# 3. Make sure psql/pg_restore are available

# They come with PostgreSQL installation (in C:\Program Files\PostgreSQL\15\bin or similar).

# If psql is not recognized, add that bin folder to your system PATH or navigate to it in cmd before running the commands.

# ‚ö° Example for your case (with your .env info):

# psql -h localhost -U postgres -d testdb -f "C:\Users\YourName\Desktop\backup.sql"

# It will then ask for your password ‚Üí enter¬†Rawat2002.

# //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

# Situation	What to do
# Added dependency locally	- docker compose up --build
# Want to add inside container (temporary)- 	docker exec -it ts_app_dev npm install <pkg>
# Didn‚Äôt change dependencies, just code	- docker compose up (no rebuild needed)
# Added dependency + changed package.json- 	docker compose build or --build flag
